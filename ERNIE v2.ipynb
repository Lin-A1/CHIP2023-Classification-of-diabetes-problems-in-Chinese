{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db40d76f-9385-40f7-a1c3-cc1929fcab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tTraining Loss: 1.9971\tTraining Accuracy: 0.5933\tValidation Accuracy: 0.8140\n",
      "Epoch 2:\tTraining Loss: 0.5674\tTraining Accuracy: 0.8458\tValidation Accuracy: 0.8770\n",
      "Epoch 3:\tTraining Loss: 0.3947\tTraining Accuracy: 0.8923\tValidation Accuracy: 0.8520\n",
      "Epoch 4:\tTraining Loss: 0.3006\tTraining Accuracy: 0.9193\tValidation Accuracy: 0.8640\n",
      "Epoch 5:\tTraining Loss: 0.2226\tTraining Accuracy: 0.9415\tValidation Accuracy: 0.8580\n",
      "Epoch 6:\tTraining Loss: 0.1675\tTraining Accuracy: 0.9567\tValidation Accuracy: 0.8750\n",
      "Epoch 7:\tTraining Loss: 0.1154\tTraining Accuracy: 0.9720\tValidation Accuracy: 0.8760\n",
      "Epoch 8:\tTraining Loss: 0.0936\tTraining Accuracy: 0.9783\tValidation Accuracy: 0.8730\n",
      "Epoch 9:\tTraining Loss: 0.0682\tTraining Accuracy: 0.9865\tValidation Accuracy: 0.8730\n",
      "Epoch 10:\tTraining Loss: 0.0637\tTraining Accuracy: 0.9873\tValidation Accuracy: 0.8710\n",
      "Epoch 11:\tTraining Loss: 0.0528\tTraining Accuracy: 0.9897\tValidation Accuracy: 0.8710\n",
      "Epoch 12:\tTraining Loss: 0.0448\tTraining Accuracy: 0.9907\tValidation Accuracy: 0.8710\n",
      "Epoch 13:\tTraining Loss: 0.0529\tTraining Accuracy: 0.9902\tValidation Accuracy: 0.8710\n",
      "Epoch 14:\tTraining Loss: 0.0626\tTraining Accuracy: 0.9888\tValidation Accuracy: 0.8710\n",
      "Epoch 15:\tTraining Loss: 0.0480\tTraining Accuracy: 0.9898\tValidation Accuracy: 0.8710\n",
      "Epoch 16:\tTraining Loss: 0.0530\tTraining Accuracy: 0.9893\tValidation Accuracy: 0.8710\n",
      "Epoch 17:\tTraining Loss: 0.0579\tTraining Accuracy: 0.9882\tValidation Accuracy: 0.8710\n",
      "Epoch 18:\tTraining Loss: 0.0558\tTraining Accuracy: 0.9892\tValidation Accuracy: 0.8710\n",
      "Epoch 19:\tTraining Loss: 0.0426\tTraining Accuracy: 0.9908\tValidation Accuracy: 0.8710\n",
      "Epoch 20:\tTraining Loss: 0.0573\tTraining Accuracy: 0.9895\tValidation Accuracy: 0.8710\n",
      "Training finished. Final Evaluation on Dev Set:\n",
      "Validation Accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 设置PyTorch随机种子\n",
    "seed = 1314\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设置Python的随机种子\n",
    "random.seed(seed)\n",
    "\n",
    "# 设置NumPy的随机种子\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 加载数据\n",
    "train_df = pd.read_table('input/train.txt', header=None)\n",
    "dev_df = pd.read_table('input/dev.txt', header=None)\n",
    "\n",
    "# 设置模型和tokenizer\n",
    "model_name = \"model/ernie-1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # 冻结ernie模型参数\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "\n",
    "# # 解冻部分ernie模型参数\n",
    "# for param in model.encoder.layer[-2:].parameters():\n",
    "#     param.requires_grad_(False)\n",
    "\n",
    "# for param in model.encoder.layer[:2].parameters():\n",
    "#     param.requires_grad_(False)\n",
    "\n",
    "# 添加Dropout层\n",
    "dropout_rate = 0.1\n",
    "model.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df, tokenizer, max_length=128):\n",
    "    texts = df[0].tolist()\n",
    "    labels = df[1].tolist()\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels)\n",
    "    return inputs, labels\n",
    "\n",
    "train_inputs, train_labels = preprocess_data(train_df, tokenizer)\n",
    "dev_inputs, dev_labels = preprocess_data(dev_df, tokenizer)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataset = TensorDataset(dev_inputs['input_ids'], dev_inputs['attention_mask'], dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义优化器和学习率调度器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader) * 1, num_training_steps=len(train_loader) * 10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0]  # 使用[CLS]的表示进行分类\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 计算损失\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 计算准确率\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "    # 计算平均损失和准确率\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 输出训练集的损失和准确率\n",
    "    print(f\"Epoch {epoch + 1}:\", end='\\t')\n",
    "    print(f\"Training Loss: {average_loss:.4f}\", end='\\t')\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\", end='\\t')\n",
    "\n",
    "    # 在每个训练周期结束后评估模型并输出验证集的准确率\n",
    "    model.eval()\n",
    "    dev_preds = []\n",
    "    dev_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.last_hidden_state[:, 0]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            dev_preds.extend(preds.cpu().numpy())\n",
    "            dev_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    dev_accuracy = accuracy_score(dev_true, dev_preds)\n",
    "    print(f\"Validation Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "# 最后输出模型的评估结果\n",
    "print(\"Training finished. Final Evaluation on Dev Set:\")\n",
    "dev_accuracy = accuracy_score(dev_true, dev_preds)\n",
    "print(f\"Validation Accuracy: {dev_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1476aac6-e37b-4b07-be6a-9ea3456dd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_table('input/pred.txt', header=None)\n",
    "\n",
    "test_df[1] = 0\n",
    "\n",
    "test_inputs, test_labels = preprocess_data(test_df, tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.last_hidden_state[:, 0]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "test_df[1] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09a9dd0-4973-4570-b094-281bff1c5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试数据的第一列\n",
    "text = test_df[0]\n",
    "label = test_df[1]\n",
    "\n",
    "# 指定要保存的文件名\n",
    "output_file = f\"output/res{dev_accuracy}.txt\"\n",
    "\n",
    "# 打开文件并将数据写入\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    for text,label in zip(text,label):\n",
    "        file.write(str(text) + \"\\t\")\n",
    "        file.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699e1ce-1843-42c6-98fe-3961ace6cc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
