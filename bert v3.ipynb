{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6d148-1a27-4de7-ab96-fb1e2b810e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebff4eb-54f5-486c-943d-76785eccaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置PyTorch随机种子\n",
    "seed = 5201314\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设置Python的随机种子\n",
    "random.seed(seed)\n",
    "\n",
    "# 设置NumPy的随机种子\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c861eb9d-6c26-41da-ac08-9999fad9a235",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model/bert_base_chinese/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_df = pd.read_table('input/train.txt', header=None)\n",
    "dev_df = pd.read_table('input/dev.txt', header=None)\n",
    "\n",
    "# 设置模型和tokenizer\n",
    "model_name = \"model/bert_base_chinese/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "# 冻结ernie模型参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# 解冻部分ernie模型参数\n",
    "for param in model.bert.encoder.layer[-4:].parameters():\n",
    "    param.requires_grad_(True)\n",
    "\n",
    "for param in model.bert.encoder.layer[:4].parameters():\n",
    "    param.requires_grad_(True)\n",
    "\n",
    "# 添加Dropout层\n",
    "dropout_rate = 0.2\n",
    "model.classifier.dropout = nn.Dropout(p=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cda67db-351b-4a3d-8902-9e7d13abcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def preprocess_data(df, tokenizer, max_length=64):\n",
    "    texts = df[0].tolist()\n",
    "    labels = df[1].tolist()\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels)\n",
    "    return inputs, labels\n",
    "\n",
    "train_inputs, train_labels = preprocess_data(train_df, tokenizer)\n",
    "dev_inputs, dev_labels = preprocess_data(dev_df, tokenizer)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataset = TensorDataset(dev_inputs['input_ids'], dev_inputs['attention_mask'], dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e49a087-7e51-4383-ac3d-b345000c2a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tTraining Loss: 0.8224\tTraining Accuracy: 0.7262\tValidation Accuracy: 0.8290\n",
      "Epoch 2:\tTraining Loss: 0.4465\tTraining Accuracy: 0.8608\tValidation Accuracy: 0.8340\n",
      "Epoch 3:\tTraining Loss: 0.3231\tTraining Accuracy: 0.8985\tValidation Accuracy: 0.8460\n",
      "Epoch 4:\tTraining Loss: 0.2346\tTraining Accuracy: 0.9280\tValidation Accuracy: 0.8570\n",
      "Epoch 5:\tTraining Loss: 0.1679\tTraining Accuracy: 0.9528\tValidation Accuracy: 0.8490\n",
      "Epoch 6:\tTraining Loss: 0.1269\tTraining Accuracy: 0.9662\tValidation Accuracy: 0.8580\n",
      "Epoch 7:\tTraining Loss: 0.0909\tTraining Accuracy: 0.9762\tValidation Accuracy: 0.8580\n",
      "Epoch 8:\tTraining Loss: 0.0571\tTraining Accuracy: 0.9878\tValidation Accuracy: 0.8560\n",
      "Epoch 9:\tTraining Loss: 0.0428\tTraining Accuracy: 0.9925\tValidation Accuracy: 0.8610\n",
      "Epoch 10:\tTraining Loss: 0.0315\tTraining Accuracy: 0.9952\tValidation Accuracy: 0.8580\n",
      "Training finished. Final Evaluation on Dev Set:\n",
      "Validation Accuracy: 0.8580\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器和学习率调度器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=8e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader) * 1, num_training_steps=len(train_loader) * 10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 计算损失\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 计算准确率\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "    # 计算平均损失和准确率\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 输出训练集的损失和准确率\n",
    "    print(f\"Epoch {epoch + 1}:\",end='\\t')\n",
    "    print(f\"Training Loss: {average_loss:.4f}\",end='\\t')\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\",end='\\t')\n",
    "\n",
    "    # 在每个训练周期结束后评估模型并输出验证集的准确率\n",
    "    model.eval()\n",
    "    dev_preds = []\n",
    "    dev_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            dev_preds.extend(preds.cpu().numpy())\n",
    "            dev_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    dev_accuracy = accuracy_score(dev_true, dev_preds)\n",
    "    print(f\"Validation Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "# 最后输出模型的评估结果\n",
    "print(\"Training finished. Final Evaluation on Dev Set:\")\n",
    "dev_accuracy = accuracy_score(dev_true, dev_preds)\n",
    "print(f\"Validation Accuracy: {dev_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4181d0-0eef-4409-91c1-4169361d297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_table('input/pred.txt', header=None)\n",
    "\n",
    "test_df[1] = 0\n",
    "\n",
    "test_inputs, test_labels = preprocess_data(test_df, tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "test_df[1] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5d103a-5268-46d4-a4e4-029d733505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试数据的第一列\n",
    "text = test_df[0]\n",
    "label = test_df[1]\n",
    "\n",
    "# 指定要保存的文件名\n",
    "output_file = f\"output/res{dev_accuracy}.txt\"\n",
    "\n",
    "# 打开文件并将数据写入\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    for text,label in zip(text,label):\n",
    "        file.write(str(text) + \"\\t\")\n",
    "        file.write(str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ccc70-f1c5-4c9d-ae1f-9347a9d69297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
